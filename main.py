import logging
import os
import asyncio
from datetime import datetime
from typing import Optional, Any
import uuid
from collections import deque

from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI, OpenAIError, APITimeoutError
import numpy as np
import pickle

# Redis for queue management
try:
    import redis.asyncio as redis
    from redis.asyncio import Redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    Redis = Any
    logging.warning("redis package not installed. Using in-memory queue.")

# ================================================================================
# Logging Configuration
# ================================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="REXA - Real Estate Expert Assistant",
    description="Solar API + RAG chatbot for real estate",
    version="1.0.0"
)

# ================================================================================
# Configuration & Global Variables
# ================================================================================

# Redis Configuration
REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
REDIS_DB = int(os.getenv("REDIS_DB", 0))
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", None)

# Health Check Configuration
HEALTH_CHECK_INTERVAL = int(os.getenv("HEALTH_CHECK_INTERVAL", 5))
MAX_UNHEALTHY_COUNT = int(os.getenv("MAX_UNHEALTHY_COUNT", 3))

# Queue Configuration
WEBHOOK_QUEUE_NAME = "rexa:webhook_queue"
WEBHOOK_PROCESSING_QUEUE = "rexa:processing_queue"
WEBHOOK_FAILED_QUEUE = "rexa:failed_queue"
MAX_RETRY_ATTEMPTS = int(os.getenv("MAX_RETRY_ATTEMPTS", 3))
QUEUE_PROCESS_INTERVAL = int(os.getenv("QUEUE_PROCESS_INTERVAL", 5))

# API Timeout Configuration
# Allowed location names (whitelist)
API_TIMEOUT = int(os.getenv("API_TIMEOUT", 4))  # Ïπ¥Ïπ¥Ïò§ÌÜ° 5Ï¥à Ï†úÌïú Í≥†Î†§

# Global state
redis_client: Optional[Any] = None
server_healthy = True
unhealthy_count = 0
last_health_check = datetime.now()

# In-memory queue fallback
in_memory_webhook_queue: deque = deque()
in_memory_processing_queue: deque = deque()
in_memory_failed_queue: deque = deque()
use_in_memory_queue = False

# ================================================================================
# Upstage Solar API Configuration
# ================================================================================

client = OpenAI(
    api_key=os.getenv("UPSTAGE_API_KEY"),
    base_url="https://api.upstage.ai/v1/solar",
    timeout=API_TIMEOUT
)

logger.info("‚úÖ Upstage Solar API client configured")

# ================================================================================
# RAG - Load Embeddings
# ================================================================================

article_chunks = []
chunk_embeddings = []
chunk_metadata = []  # Îß§Î¨º ÌÉÄÏûÖ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä

try:
    with open("embeddings.pkl", "rb") as f:
        data = pickle.load(f)
        article_chunks = data["chunks"]
        chunk_embeddings = data["embeddings"]
        # Îß§Î¨º ÌÉÄÏûÖ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÏúºÎ©¥ Î°úÎìú, ÏóÜÏúºÎ©¥ Îπà Î¶¨Ïä§Ìä∏
        chunk_metadata = data.get("metadata", [])
    logger.info(f"‚úÖ Loaded {len(article_chunks)} chunks from embeddings.pkl")
    logger.info(f"‚úÖ RAG is ENABLED with {len(article_chunks)} chunks")
    logger.info(f"‚úÖ Metadata loaded: {len(chunk_metadata)} entries")
except FileNotFoundError:
    logger.warning("‚ö†Ô∏è embeddings.pkl not found - RAG will not be available")
    logger.warning("‚ö†Ô∏è Server will continue WITHOUT RAG - responses will be general")
    logger.warning("‚ö†Ô∏è To enable RAG: run 'python embedding2_solar.py' and redeploy")
except Exception as e:
    logger.error(f"‚ùå Failed to load embeddings: {e}")
    logger.warning("‚ö†Ô∏è Server will continue WITHOUT RAG")

# ================================================================================
# ÎèôÏ†ÅÏúºÎ°ú Î≥¥Ïú† ÏßÄÏó≠ Ï∂îÏ∂ú (Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò)
# ================================================================================

def extract_locations_from_metadata(metadata_list):
    """Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ÏóêÏÑú Î≥¥Ïú† ÏßÄÏó≠Î™ÖÏùÑ Ï∂îÏ∂ú"""
    locations = set()
    
    # ÏßÄÏó≠Î™Ö Ï∂îÏ∂ú Ìå®ÌÑ¥
    import re
    
    for meta in metadata_list:
        if meta.get("type") in ["TYPE_A", "TYPE_B"]:
            name = meta.get("name", "")
            address = meta.get("address", "")
            
            # addressÏóêÏÑú Ï∂îÏ∂ú (TYPE_A)
            if address:
                # "ÏÑúÏö∏ÌäπÎ≥ÑÏãú Í∞ïÎÇ®Íµ¨ ÌïôÎèôÎ°ú 401" ‚Üí "Í∞ïÎÇ®Íµ¨"
                # "ÏÑúÏö∏ ÎßàÌè¨Íµ¨ ÏÑúÍµêÎèô 328-26" ‚Üí "ÎßàÌè¨Íµ¨", "ÏÑúÍµêÎèô"
                dong_match = re.search(r'([Í∞Ä-Ìû£]+Îèô)', address)
                gu_match = re.search(r'([Í∞Ä-Ìû£]+Íµ¨)', address)
                if dong_match:
                    locations.add(dong_match.group(1))
                if gu_match:
                    locations.add(gu_match.group(1))
            
            # nameÏóêÏÑú Ï∂îÏ∂ú (TYPE_B)
            # "ÏÜåÎã¥ÎπåÎî© (Ï≤≠Îã¥Îèô 39-7)" ‚Üí "Ï≤≠Îã¥Îèô"
            # "ÎçîÎ≤†Ïä§Ìä∏ Ïã†Í∏∏Îèô (342-337)" ‚Üí "Ïã†Í∏∏Îèô"
            dong_match = re.search(r'([Í∞Ä-Ìû£]+Îèô)', name)
            gu_match = re.search(r'([Í∞Ä-Ìû£]+Íµ¨)', name)
            if dong_match:
                locations.add(dong_match.group(1))
            if gu_match:
                locations.add(gu_match.group(1))
    
    return locations

# Î≥¥Ïú† ÏßÄÏó≠ ÏûêÎèô Ï∂îÏ∂ú
ALLOWED_LOCATIONS = extract_locations_from_metadata(chunk_metadata)

# ÏÑúÏö∏ Ï†ÑÏ≤¥ Ï£ºÏöî ÏßÄÏó≠ (Ï∞∏Í≥†Ïö©)
# Ïó≠Î™Ö ‚Üí ÏßÄÏó≠ Îß§Ìïë (Ïó≠ Í∑ºÏ≤ò = Ìï¥Îãπ Îèô)
STATION_TO_LOCATION = {
    # Í∞ïÎÇ®Í∂å
    "Í∞ïÎÇ®Íµ¨Ï≤≠Ïó≠": ["Ï≤≠Îã¥Îèô", "ÎÖºÌòÑÎèô"],
    "Ï≤≠Îã¥Ïó≠": ["Ï≤≠Îã¥Îèô"],
    "Í∞ïÎÇ®Ïó≠": ["Ïó≠ÏÇºÎèô", "ÎÖºÌòÑÎèô"],
    "ÏÑ†Î¶âÏó≠": ["Ïó≠ÏÇºÎèô", "ÎåÄÏπòÎèô"],
    "ÏÇºÏÑ±Ïó≠": ["ÏÇºÏÑ±Îèô"],
    # ÏòÅÎì±Ìè¨Í∂å
    "ÏòÅÎì±Ìè¨Íµ¨Ï≤≠Ïó≠": ["ÎãπÏÇ∞Îèô", "Ïã†Í∏∏Îèô"],
    "Ïã†Í∏∏Ïó≠": ["Ïã†Í∏∏Îèô"],
    "ÏòÅÎì±Ìè¨Ïó≠": ["ÏòÅÎì±Ìè¨Îèô", "Ïã†Í∏∏Îèô"],
    "ÎãπÏÇ∞Ïó≠": ["ÎãπÏÇ∞Îèô", "ÏñëÌèâÎèô"],
    "Î¨∏ÎûòÏó≠": ["Î¨∏ÎûòÎèô"],
    # Ï§ëÎûëÍ∂å
    "Ïã†ÎÇ¥Ïó≠": ["Ïã†ÎÇ¥Îèô"],
    "ÏÉÅÎ¥âÏó≠": ["ÏÉÅÎ¥âÎèô"],
    # ÎßàÌè¨Í∂å
    "ÌôçÎåÄÏûÖÍµ¨Ïó≠": ["ÏÑúÍµêÎèô", "Ïó∞ÎÇ®Îèô"],
    "Ìï©Ï†ïÏó≠": ["Ìï©Ï†ïÎèô", "Ïó∞ÎÇ®Îèô"],
    "ÏÉÅÏàòÏó≠": ["ÏÉÅÏàòÎèô"],
    # Í∏∞ÌÉÄ
    "Ï¢ÖÍ∞ÅÏó≠": ["Ï¢ÖÎ°ú"],
    "Ïû†ÏõêÏó≠": ["Ïû†ÏõêÎèô"],
}

# ÏùºÎ∞ò Î™ÖÏÇ¨ Ï†úÍ±∞ (ÏßÄÏó≠ Ï∂îÏ∂ú Ïãú)
GENERIC_BUILDING_TERMS = {
    "Íº¨ÎßàÎπåÎî©", "ÎπåÎî©", "Í±¥Î¨º", "Îß§Î¨º", "Î∂ÄÎèôÏÇ∞",
    "ÏÉÅÍ∞Ä", "Ïò§ÌîºÏä§ÌÖî", "ÏÇ¨Î¨¥Ïã§", "Ï†êÌè¨"
}

# ÏÑúÏö∏ Ï†ÑÏ≤¥ Ï£ºÏöî Îèô (Îèô Îã®ÏúÑÎßå - Íµ¨Îäî Ï†úÏô∏)
SEOUL_ALL_DONGS = {
    # Í∞ïÎÇ®Í∂å
    "Ï≤≠Îã¥Îèô", "ÎÖºÌòÑÎèô", "ÎåÄÏπòÎèô", "ÏÇºÏÑ±Îèô", "Ïó≠ÏÇºÎèô", "Ïã†ÏÇ¨Îèô", "ÏïïÍµ¨Ï†ïÎèô",
    # ÏÜ°ÌååÍ∂å
    "Ïû†Ïã§Îèô", "ÏÜ°ÌååÎèô", "Î∞©Ïù¥Îèô", "Î¨∏Ï†ïÎèô",
    # ÏÑúÏ¥àÍ∂å
    "Î∞òÌè¨Îèô", "ÏÑúÏ¥àÎèô", "Î∞©Î∞∞Îèô", "ÏñëÏû¨Îèô",
    # ÏòÅÎì±Ìè¨Í∂å
    "Ïã†Í∏∏Îèô", "ÏñëÌèâÎèô", "Î¨∏ÎûòÎèô", "ÎãπÏÇ∞Îèô", "Ïó¨ÏùòÎèÑÎèô",
    # Ï§ëÎûëÍ∂å
    "Ïã†ÎÇ¥Îèô", "ÏÉÅÎ¥âÎèô", "ÎßùÏö∞Îèô", "Ï§ëÌôîÎèô",
    # ÎßàÌè¨Í∂å
    "ÏÑúÍµêÎèô", "Ïó∞ÎÇ®Îèô", "ÏÉÅÏàòÎèô", "Ìï©Ï†ïÎèô", "ÎßùÏõêÎèô",
    # Ïö©ÏÇ∞Í∂å
    "Ïù¥ÌÉúÏõê", "ÌïúÎÇ®Îèô", "Ïö©ÏÇ∞Îèô",
    # Í∏∞ÌÉÄ
    "Ïû†ÏõêÎèô", "Ïã†Î¶ºÎèô", "ÏãúÌù•Îèô", "Î¥âÏ≤úÎèô",
    "ÏÑ±ÎÇ¥Îèô", "Ï†úÍ∏∞Îèô", "Î™ÖÎèô", "ÏùÑÏßÄÎ°ú"
}

# Í∏àÏßÄ ÏßÄÏó≠ = ÏÑúÏö∏ Ï†ÑÏ≤¥ Îèô - Î≥¥Ïú† ÏßÄÏó≠ (Îèô Îã®ÏúÑÎßå!)
FORBIDDEN_LOCATIONS = SEOUL_ALL_DONGS - ALLOWED_LOCATIONS

# ÏßÄÏó≠Î≥Ñ Í∞ÄÍ≤©ÎåÄ (ÏñµÏõê Îã®ÏúÑ)
LOCATION_PRICE_RANGES = {
    "Ï≤≠Îã¥Îèô": (100, 1000),    # 100Ïñµ~1000Ïñµ
    "ÎÖºÌòÑÎèô": (100, 1000),    # 100Ïñµ~1000Ïñµ  
    "ÎåÄÏπòÎèô": (100, 2000),    # 100Ïñµ~2000Ïñµ
    "Ïã†ÎÇ¥Îèô": (5, 20),        # 5Ïñµ~20Ïñµ
    "ÏÉÅÎ¥âÎèô": (5, 20),        # 5Ïñµ~20Ïñµ
    "Ïã†Í∏∏Îèô": (10, 50),       # 10Ïñµ~50Ïñµ
    "ÏñëÌèâÎèô": (5, 20),        # 5Ïñµ~20Ïñµ
    "Î¨∏ÎûòÎèô": (3, 10),        # 3Ïñµ~10Ïñµ
    "ÏÑúÍµêÎèô": (50, 150),      # 50Ïñµ~150Ïñµ
    "Ïû†ÏõêÎèô": (10, 50),       # 10Ïñµ~50Ïñµ
    "Ï¢ÖÎ°ú": (5, 20),          # 5Ïñµ~20Ïñµ
    "Ïã†Î¶ºÎèô": (3, 10),        # 3Ïñµ~10Ïñµ
    "ÏãúÌù•Îèô": (3, 10),        # 3Ïñµ~10Ïñµ
}

# ÌóàÏö©Îêú Í±¥Î¨º Ïö©ÎèÑ (Î≥¥Ïú† Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò)
ALLOWED_BUILDING_TYPES = {
    "Ï†ú1Ï¢ÖÍ∑ºÎ¶∞ÏÉùÌôúÏãúÏÑ§", "Ï†ú2Ï¢ÖÍ∑ºÎ¶∞ÏÉùÌôúÏãúÏÑ§", "Í∑ºÎ¶∞ÏÉùÌôúÏãúÏÑ§",
    "ÏóÖÎ¨¥ÏãúÏÑ§", "Ïò§ÌîºÏä§ÌÖî", "Í∑ºÎ¶∞ÏÉÅÍ∞Ä", "ÏÉÅÍ∞ÄÍ±¥Î¨º", "ÏÇ¨Î¨¥Ïã§",
    "Íº¨ÎßàÎπåÎî©", "ÎπåÎî©", "Í±¥Î¨º", "ÌÜ†ÏßÄ", "ÎåÄÏßÄ"
}

# Í∏àÏßÄÎêú Í±¥Î¨º Ïö©ÎèÑ (Î≥¥Ïú† Îç∞Ïù¥ÌÑ∞Ïóê ÏóÜÏùå)
FORBIDDEN_BUILDING_TYPES = {
    "Îã§Í∞ÄÍµ¨Ï£ºÌÉù", "Îã®ÎèÖÏ£ºÌÉù", "Îã§ÏÑ∏ÎåÄÏ£ºÌÉù", "ÏïÑÌååÌä∏", "ÎπåÎùº",
    "Ïó∞Î¶ΩÏ£ºÌÉù", "Ï£ºÌÉù", "Ï£ºÍ±∞Ïö©", "ÏõêÎ£∏", "Ìà¨Î£∏"
}

logger.info(f"‚úÖ Î≥¥Ïú† ÏßÄÏó≠ ÏûêÎèô Ï∂îÏ∂ú: {len(ALLOWED_LOCATIONS)}Í∞ú - {sorted(ALLOWED_LOCATIONS)}")
logger.info(f"‚ö†Ô∏è Í∏àÏßÄ ÏßÄÏó≠ ÏûêÎèô ÏÉùÏÑ± (Îèô Îã®ÏúÑÎßå): {len(FORBIDDEN_LOCATIONS)}Í∞ú")
logger.info(f"üîç Í∏àÏßÄ ÏßÄÏó≠ ÏÉòÌîå: {list(sorted(FORBIDDEN_LOCATIONS))[:10]}")
logger.info(f"üí∞ ÏßÄÏó≠Î≥Ñ Í∞ÄÍ≤©ÎåÄ ÏÑ§Ï†ï: {len(LOCATION_PRICE_RANGES)}Í∞ú ÏßÄÏó≠")
logger.info(f"üè¢ ÌóàÏö© Í±¥Î¨º Ïö©ÎèÑ: {len(ALLOWED_BUILDING_TYPES)}Í∞ú")
logger.info(f"üö´ Í∏àÏßÄ Í±¥Î¨º Ïö©ÎèÑ: {len(FORBIDDEN_BUILDING_TYPES)}Í∞ú")

# ================================================================================
# RAG Helper Functions
# ================================================================================

def cosine_similarity(a, b):
    """Calculate cosine similarity between two vectors"""
    from numpy import dot
    from numpy.linalg import norm
    return dot(a, b) / (norm(a) * norm(b))

async def get_relevant_context(prompt: str, top_n: int = 2) -> dict:
    """Get relevant context from embeddings for RAG
    Returns: {
        'context': str,
        'property_type': str,  # 'TYPE_A' or 'TYPE_B'
        'property_name': str
    }
    """
    if not chunk_embeddings or not article_chunks:
        logger.warning("‚ö†Ô∏è No embeddings available for RAG")
        return {"context": "", "property_type": "UNKNOWN", "property_name": ""}
    
    try:
        # ÏûÑÎ≤†Îî© Ï∞®Ïõê ÏûêÎèô Í∞êÏßÄ
        embedding_dim = len(chunk_embeddings[0])
        logger.info(f"üìä Detected embedding dimension: {embedding_dim}")
        
        # Ï∞®ÏõêÏóê Îî∞Îùº Ï†ÅÏ†àÌïú API ÏÇ¨Ïö©
        if embedding_dim == 1536:
            # OpenAI ÏûÑÎ≤†Îî© (text-embedding-3-small)
            logger.info("üîß Using OpenAI embedding model")
            try:
                openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
                q_embedding = openai_client.embeddings.create(
                    input=prompt, 
                    model="text-embedding-3-small"
                ).data[0].embedding
            except Exception as e:
                logger.error(f"‚ùå OpenAI embedding failed: {e}")
                logger.info("üí° Set OPENAI_API_KEY environment variable")
                return {"context": "", "property_type": "UNKNOWN", "property_name": ""}
                
        else:
            # Solar ÏûÑÎ≤†Îî© (Î™®Îì† Îã§Î•∏ Ï∞®Ïõê)
            logger.info(f"üîß Using Solar embedding model (dimension: {embedding_dim})")
            try:
                q_embedding = client.embeddings.create(
                    input=prompt, 
                    model="solar-embedding-1-large-query"  # Solar ÏøºÎ¶¨Ïö© Î™®Îç∏
                ).data[0].embedding
            except Exception as e:
                logger.error(f"‚ùå Solar embedding failed: {e}")
                logger.error(f"   Model: solar-embedding-1-large-query")
                return {"context": "", "property_type": "UNKNOWN", "property_name": ""}
        
        # Calculate similarities
        similarities = [cosine_similarity(q_embedding, emb) for emb in chunk_embeddings]
        
        # Get top N most similar chunks
        top_indices = np.argsort(similarities)[-top_n:][::-1]
        selected_context = "\n\n".join([article_chunks[i] for i in top_indices])
        
        # Îß§Î¨º ÌÉÄÏûÖ ÌåêÎã® (Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÏúºÎ©¥ ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ ÌÖçÏä§Ìä∏ Í∏∞Î∞ò ÌåêÎã®)
        property_type = "TYPE_B"  # Í∏∞Î≥∏Í∞í: ÎπÑÏ†úÌú¥ Ï§ëÍ∞úÏÇ¨ Îß§Î¨º
        property_name = ""
        
        if chunk_metadata and len(chunk_metadata) > top_indices[0]:
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÏúºÎ©¥ ÏÇ¨Ïö©
            meta = chunk_metadata[top_indices[0]]
            property_type = meta.get("type", "TYPE_B")
            property_name = meta.get("name", "")
            logger.info(f"‚úÖ Using metadata: {property_type} - {property_name}")
        else:
            # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏúºÎ©¥ ÌÖçÏä§Ìä∏ Í∏∞Î∞ò ÌåêÎã®
            top_chunk = article_chunks[top_indices[0]]
            if "Í∏àÌïòÎπåÎî©" in top_chunk and "ÏÑúÏïàÍ∞úÎ∞ú" in top_chunk:
                property_type = "TYPE_A"
                property_name = "Í∏àÌïòÎπåÎî©"
                logger.info(f"‚úÖ Detected TYPE_A (ÏÑúÏïàÍ∞úÎ∞ú Î≥¥Ïú†): {property_name}")
            else:
                # Îß§Î¨ºÎ™Ö Ï∂îÏ∂ú ÏãúÎèÑ
                for line in top_chunk.split('\n'):
                    if 'Í±¥Î¨º' in line or 'Îß§Î¨º' in line:
                        property_name = line.split(':')[0].strip() if ':' in line else ""
                        break
                logger.info(f"‚úÖ Detected TYPE_B (ÎπÑÏ†úÌú¥ Îß§Î¨º): {property_name}")
        
        # Format similarities for logging
        similarity_scores = [f"{similarities[i]:.3f}" for i in top_indices]
        logger.info(f"‚úÖ Retrieved {top_n} relevant chunks (similarities: {similarity_scores})")
        
        return {
            "context": selected_context,
            "property_type": property_type,
            "property_name": property_name
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error getting relevant context: {e}")
        return {"context": "", "property_type": "UNKNOWN", "property_name": ""}

# ================================================================================
# Pydantic Models
# ================================================================================

class DetailParams(BaseModel):
    prompt: dict

class Action(BaseModel):
    params: dict
    detailParams: dict

class RequestBody(BaseModel):
    action: Action

class QueuedRequest(BaseModel):
    request_id: str
    request_body: dict
    timestamp: str
    retry_count: int = 0
    error_message: Optional[str] = None

class HealthStatus(BaseModel):
    status: str
    model: str
    mode: str
    server_healthy: bool
    last_check: str
    redis_connected: bool
    queue_size: int
    processing_queue_size: int
    failed_queue_size: int

# ================================================================================
# Redis & Queue Management
# ================================================================================

async def init_redis():
    """Initialize Redis connection"""
    global redis_client, use_in_memory_queue
    
    if not REDIS_AVAILABLE:
        logger.warning("‚ö†Ô∏è Redis package not installed - using in-memory queue")
        use_in_memory_queue = True
        return
    
    try:
        redis_client = await redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=REDIS_DB,
            password=REDIS_PASSWORD,
            decode_responses=True,
            socket_connect_timeout=5,
            socket_keepalive=True,
            retry_on_timeout=True
        )
        await redis_client.ping()
        logger.info(f"‚úÖ Redis connected: {REDIS_HOST}:{REDIS_PORT}")
        use_in_memory_queue = False
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Redis connection failed: {e}")
        logger.warning("‚ö†Ô∏è Using in-memory queue instead")
        use_in_memory_queue = True

async def close_redis():
    """Close Redis connection"""
    global redis_client
    if redis_client and not use_in_memory_queue:
        await redis_client.close()
        logger.info("‚úÖ Redis connection closed")

async def enqueue_webhook_request(request_id: str, request_body: dict):
    """Enqueue a webhook request for later processing"""
    global in_memory_webhook_queue  # Ï†ÑÏó≠ Î≥ÄÏàò ÏÑ†Ïñ∏
    try:
        queued_request = QueuedRequest(
            request_id=request_id,
            request_body=request_body,
            timestamp=datetime.now().isoformat(),
            retry_count=0
        )
        
        if use_in_memory_queue:
            in_memory_webhook_queue.append(queued_request)
            logger.info(f"‚úÖ Enqueued to in-memory queue: {request_id}")
        else:
            if redis_client:
                await redis_client.lpush(
                    WEBHOOK_QUEUE_NAME,
                    queued_request.model_dump_json()
                )
                logger.info(f"‚úÖ Enqueued to Redis: {request_id}")
    except Exception as e:
        logger.error(f"‚ùå Failed to enqueue request: {e}")

async def dequeue_webhook_request() -> Optional[QueuedRequest]:
    """Dequeue the next webhook request"""
    global in_memory_processing_queue, in_memory_webhook_queue  # Ï†ÑÏó≠ Î≥ÄÏàò ÏÑ†Ïñ∏
    try:
        if use_in_memory_queue:
            if len(in_memory_webhook_queue) == 0:
                return None
            req = in_memory_webhook_queue.popleft()
            in_memory_processing_queue.append(req)
            return req
        
        if not redis_client:
            return None
        
        request_json = await redis_client.rpoplpush(
            WEBHOOK_QUEUE_NAME,
            WEBHOOK_PROCESSING_QUEUE
        )
        
        if not request_json:
            return None
        
        return QueuedRequest.model_validate_json(request_json)
        
    except Exception as e:
        logger.error(f"‚ùå Failed to dequeue request: {e}")
        return None

async def complete_webhook_request(request_id: str):
    """Mark a webhook request as completed"""
    global in_memory_processing_queue  # Ï†ÑÏó≠ Î≥ÄÏàò ÏÑ†Ïñ∏ Ï∂îÍ∞Ä
    try:
        if use_in_memory_queue:
            in_memory_processing_queue = deque([
                req for req in in_memory_processing_queue 
                if req.request_id != request_id
            ])
        else:
            if redis_client:
                items = await redis_client.lrange(WEBHOOK_PROCESSING_QUEUE, 0, -1)
                for item in items:
                    req = QueuedRequest.model_validate_json(item)
                    if req.request_id == request_id:
                        await redis_client.lrem(WEBHOOK_PROCESSING_QUEUE, 1, item)
                        break
    except Exception as e:
        logger.error(f"‚ùå Failed to complete request: {e}")

async def fail_webhook_request(request_id: str, error_message: str):
    """Move a failed webhook request to the failed queue"""
    global in_memory_processing_queue, in_memory_failed_queue, in_memory_webhook_queue  # Ï†ÑÏó≠ Î≥ÄÏàò ÏÑ†Ïñ∏
    try:
        if use_in_memory_queue:
            for req in in_memory_processing_queue:
                if req.request_id == request_id:
                    req.retry_count += 1
                    req.error_message = error_message
                    
                    if req.retry_count >= MAX_RETRY_ATTEMPTS:
                        in_memory_failed_queue.append(req)
                        in_memory_processing_queue.remove(req)
                    else:
                        in_memory_webhook_queue.appendleft(req)
                        in_memory_processing_queue.remove(req)
                    break
        else:
            if redis_client:
                items = await redis_client.lrange(WEBHOOK_PROCESSING_QUEUE, 0, -1)
                for item in items:
                    req = QueuedRequest.model_validate_json(item)
                    if req.request_id == request_id:
                        req.retry_count += 1
                        req.error_message = error_message
                        
                        await redis_client.lrem(WEBHOOK_PROCESSING_QUEUE, 1, item)
                        
                        if req.retry_count >= MAX_RETRY_ATTEMPTS:
                            await redis_client.lpush(
                                WEBHOOK_FAILED_QUEUE,
                                req.model_dump_json()
                            )
                        else:
                            await redis_client.lpush(
                                WEBHOOK_QUEUE_NAME,
                                req.model_dump_json()
                            )
                        break
    except Exception as e:
        logger.error(f"‚ùå Failed to fail request: {e}")

async def get_queue_sizes():
    """Get current queue sizes"""
    try:
        if use_in_memory_queue:
            return (
                len(in_memory_webhook_queue),
                len(in_memory_processing_queue),
                len(in_memory_failed_queue)
            )
        
        if not redis_client:
            return (0, 0, 0)
        
        queue_size = await redis_client.llen(WEBHOOK_QUEUE_NAME)
        processing_size = await redis_client.llen(WEBHOOK_PROCESSING_QUEUE)
        failed_size = await redis_client.llen(WEBHOOK_FAILED_QUEUE)
        
        return (queue_size, processing_size, failed_size)
        
    except Exception as e:
        logger.error(f"‚ùå Failed to get queue sizes: {e}")
        return (0, 0, 0)

# ================================================================================
# Background Tasks
# ================================================================================

async def health_check_monitor():
    """Monitor Solar API health"""
    global server_healthy, unhealthy_count, last_health_check
    
    logger.info("üè• Health check monitor started")
    
    while True:
        try:
            await asyncio.sleep(HEALTH_CHECK_INTERVAL)
            
            # Test Solar API
            test_response = client.chat.completions.create(
                model="solar-mini",
                messages=[{"role": "user", "content": "ping"}],
                max_tokens=5,
                timeout=2
            )
            
            if test_response.choices[0].message.content:
                if not server_healthy:
                    logger.info("‚úÖ Server recovered - healthy")
                server_healthy = True
                unhealthy_count = 0
            else:
                raise Exception("Empty response from API")
                
        except Exception as e:
            unhealthy_count += 1
            logger.warning(f"‚ö†Ô∏è Health check failed ({unhealthy_count}/{MAX_UNHEALTHY_COUNT}): {e}")
            
            if unhealthy_count >= MAX_UNHEALTHY_COUNT:
                server_healthy = False
                logger.error("‚ùå Server marked as unhealthy")
        
        finally:
            last_health_check = datetime.now()

async def queue_processor():
    """Process queued webhook requests"""
    logger.info("üîÑ Queue processor started")
    
    while True:
        try:
            await asyncio.sleep(QUEUE_PROCESS_INTERVAL)
            
            request = await dequeue_webhook_request()
            if not request:
                continue
            
            logger.info(f"üì§ Processing queued request: {request.request_id}")
            
            try:
                result = await process_solar_rag_request(request.request_body)
                await complete_webhook_request(request.request_id)
                logger.info(f"‚úÖ Queued request {request.request_id} completed")
                
            except Exception as e:
                error_msg = f"{type(e).__name__}: {str(e)}"
                logger.error(f"‚ùå Failed to process queued request: {error_msg}")
                await fail_webhook_request(request.request_id, error_msg)
                
        except Exception as e:
            logger.error(f"‚ùå Queue processor error: {e}")
            await asyncio.sleep(1)

# ================================================================================
# Core Request Processing with RAG
# ================================================================================

async def process_solar_rag_request(request_body: dict):
    """Process request with Solar API + RAG"""
    
    # Extract prompt from various possible locations
    prompt = None
    
    if request_body.get("action", {}).get("params", {}).get("prompt"):
        prompt = request_body["action"]["params"]["prompt"]
        logger.info(f"‚úÖ Method 1 (action.params.prompt): '{prompt}'")
    
    elif request_body.get("action", {}).get("detailParams", {}).get("prompt", {}).get("value"):
        prompt = request_body["action"]["detailParams"]["prompt"]["value"]
        logger.info(f"‚úÖ Method 2 (action.detailParams.prompt.value): '{prompt}'")
    
    elif request_body.get("userRequest", {}).get("utterance"):
        prompt = request_body["userRequest"]["utterance"]
        logger.info(f"‚úÖ Method 3 (userRequest.utterance): '{prompt}'")
    
    elif request_body.get("utterance"):
        prompt = request_body["utterance"]
        logger.info(f"‚úÖ Method 4 (utterance): '{prompt}'")
    
    if not prompt or (isinstance(prompt, str) and prompt.strip() == ""):
        logger.warning("‚ö†Ô∏è No prompt found in request!")
        return {
            "version": "2.0",
            "template": {
                "outputs": [{
                    "simpleText": {
                        "text": "ÏïàÎÖïÌïòÏÑ∏Ïöî! REXAÏûÖÎãàÎã§. Î¨¥ÏóáÏù¥ Í∂ÅÍ∏àÌïòÏã†Í∞ÄÏöî?\nÎ∂ÄÎèôÏÇ∞ ÏÑ∏Í∏à, Í≤ΩÎß§, ÎØºÎ≤ï Îì±Ïóê ÎåÄÌï¥ ÏßàÎ¨∏Ìï¥Ï£ºÏÑ∏Ïöî."
                    }
                }]
            }
        }
    
    logger.info(f"üìù Final extracted prompt: '{prompt}'")
    
    # ÏßàÎ¨∏ Ï†ÑÏ≤òÎ¶¨: Ïó≠Î™Ö ‚Üí ÏßÄÏó≠Î™Ö Î≥ÄÌôò
    original_prompt = prompt
    prompt = preprocess_query(prompt)
    if prompt != original_prompt:
        logger.info(f"üîÑ Preprocessed: '{original_prompt}' ‚Üí '{prompt}'")
    
    # Ï∂îÏ≤ú/Î¶¨Ïä§Ìä∏ ÏöîÏ≤≠ Í∞êÏßÄ (Îçî ÏóÑÍ≤©ÌïòÍ≤å)
    is_recommendation_request = any(keyword in prompt.lower() for keyword in 
                                    ["Ï∂îÏ≤ú", "Î¶¨Ïä§Ìä∏", "Î™©Î°ù", "Î™á Í∞ú", "Ïó¨Îü¨ Í∞ú", "3Í∞ú", "5Í∞ú", "10Í∞ú", 
                                     "Îß§Î¨º Î≥¥Ïó¨", "Îß§Î¨º ÏïåÎ†§"])
    
    # Ï∂îÏ≤ú ÏöîÏ≤≠Ïù¥Î©¥ Î¨¥Ï°∞Í±¥ ÌïòÎìúÏΩîÎî© Î™©Î°ù Î∞òÌôò (AI Ïö∞Ìöå)
    if is_recommendation_request:
        logger.info("üéØ Recommendation request detected - returning hardcoded list")
        return {
            "version": "2.0",
            "template": {
                "outputs": [{
                    "simpleText": {
                        "text": """ÌòÑÏû¨ Î≥¥Ïú†Ìïú Îß§Î¨º Îç∞Ïù¥ÌÑ∞Îäî Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:

[ÏÑúÏïàÍ∞úÎ∞ú Î≥¥Ïú† ÏûêÏÇ∞]
‚Ä¢ Í∏àÌïòÎπåÎî© 11Ï∏µ (Í∞ïÎÇ®Íµ¨ ÌïôÎèôÎ°ú 401) - ÏûÑÎåÄ
  Î≥¥Ï¶ùÍ∏à 3.5Ïñµ, ÏõîÏÑ∏ 2,579ÎßåÏõê
‚Ä¢ ÏÑúÍµêÎèô 328-26 (ÎßàÌè¨Íµ¨ ÏÑúÍµêÎèô) - Îß§Îß§ 80Ïñµ
  ÏàòÏùµÎ•† 2.43%

[ÏãúÏû• Ï∞∏Í≥† Ï†ïÎ≥¥ - Í∞ïÎÇ®Í∂å]
‚Ä¢ ÏÜåÎã¥ÎπåÎî© (Ï≤≠Îã¥Îèô XX) - ÏïΩ 140ÏñµÏõêÎåÄ
‚Ä¢ Ìò∏ÏïîÎπåÎî© (Ï≤≠Îã¥Îèô XX) - ÏïΩ 160ÏñµÏõêÎåÄ
‚Ä¢ Ï≤≠Îã¥Îèô XX (ÌïôÎèôÎ°ú55Í∏∏) - ÏïΩ 130ÏñµÏõêÎåÄ
‚Ä¢ Ï≤≠Îã¥Îèô XX (ÌïôÎèôÎ°ú55Í∏∏) - ÏïΩ 147ÏñµÏõêÎåÄ
‚Ä¢ ÎÇ®ÏÇ∞Îπå (ÎÖºÌòÑÎèô XX) - ÏïΩ 130ÏñµÏõêÎåÄ
‚Ä¢ ÎÖºÌòÑÎèô XX - ÏïΩ 210ÏñµÏõêÎåÄ
‚Ä¢ ÎÖºÌòÑÎèô XX - ÏïΩ 500ÏñµÏõêÎåÄ
‚Ä¢ Î≥¥ÏÑ±Îü≠Ïä§ÌÉÄÏö¥ (ÎÖºÌòÑÎèô XX) - ÏïΩ 500ÏñµÏõêÎåÄ
‚Ä¢ ÎåÄÏπòÎèô XX (ÏÑ†Î¶âÏó≠ ÌÜ†ÏßÄ) - ÏïΩ 1,160ÏñµÏõêÎåÄ

[ÏãúÏû• Ï∞∏Í≥† Ï†ïÎ≥¥ - ÏòÅÎì±Ìè¨Í∂å]
‚Ä¢ ÎçîÎ≤†Ïä§Ìä∏ Ïã†Í∏∏Îèô (ÎèÑÎ¶ºÎ°ú XX) - ÏïΩ 19ÏñµÏõêÎåÄ
‚Ä¢ Î¨∏ÎûòÎèô Ïπ¥ÌéòÍ±¥Î¨º (Î¨∏ÎûòÎ∂ÅÎ°ú XX) - ÏïΩ 4.3ÏñµÏõêÎåÄ
‚Ä¢ ÏñëÌèâÎèô ÎòêÎò£Ïò®Î∞ò (ÏòÅÎì±Ìè¨Î°ú18Í∏∏ XX) - ÏïΩ 8ÏñµÏõêÎåÄ
‚Ä¢ ÏòÅÎì±Ìè¨ Î£®ÎØ∏ÏóêÎ•¥ (ÎèÑÎ¶ºÎ°ú XX) - ÏïΩ 65ÏñµÏõêÎåÄ

[ÏãúÏû• Ï∞∏Í≥† Ï†ïÎ≥¥ - Ï§ëÎûëÍ∂å]
‚Ä¢ Ïã†ÎÇ¥Îèô Ïã†Ï∂ï Íº¨ÎßàÎπåÎî© (Ïã†ÎÇ¥Î°ú10Í∏∏ XX) - ÏïΩ 9ÏñµÏõêÎåÄ
‚Ä¢ ÏÉÅÎ¥âÎèô Ï¢ÖÌï©ÎØ∏Ïã±Ï¥ùÌåê - ÏïΩ 9.8ÏñµÏõêÎåÄ

[ÏãúÏû• Ï∞∏Í≥† Ï†ïÎ≥¥ - Í∏∞ÌÉÄ]
‚Ä¢ Ï¢ÖÎ°ú ÏñëÏßÄÏãù (Ïú®Í≥°Î°ú XX) - ÏïΩ 9.5ÏñµÏõêÎåÄ
‚Ä¢ Ïû†ÏõêÎèô ÏÉÅÍ∞Ä¬∑ÏÇ¨Î¨¥Ïã§ (Ïã†Î∞òÌè¨Î°ú47Í∏∏ XX) - ÏûÑÎåÄÏÉÅÌíà
‚Ä¢ Ïã†Î¶ºÎèô XX - ÏïΩ 4ÏñµÏõêÎåÄ
‚Ä¢ ÏãúÌù•Îèô XX - ÏïΩ 4.1ÏñµÏõêÎåÄ
‚Ä¢ ÏãúÌù•Îèô XX - ÏïΩ 3.5ÏñµÏõêÎåÄ

Íµ¨Ï≤¥Ï†ÅÏù∏ Îß§Î¨ºÎ™Ö(Ïòà: "ÏÜåÎã¥ÎπåÎî©", "Í∏àÌïòÎπåÎî© 11Ï∏µ")ÏùÑ ÎßêÏîÄÌïòÏãúÎ©¥ ÏÉÅÏÑ∏ Ï†ïÎ≥¥Î•º ÏïàÎÇ¥Ìï¥ÎìúÎ¶ΩÎãàÎã§.

üìû ÏÉÅÎã¥: ÏÑúÏïàÍ∞úÎ∞ú Ïª®ÏÑ§ÌåÖÌåÄ 02-3443-0724"""
                    }
                }]
            }
        }
    
    # Get relevant context using RAG (Îã®Ïùº Îß§Î¨º ÏöîÏ≤≠Îßå)
    rag_result = await get_relevant_context(prompt, top_n=3)  # Ï†ïÌôïÎèÑ Ïö∞ÏÑ†: 1->3
    context = rag_result["context"]
    property_type = rag_result["property_type"]
    property_name = rag_result["property_name"]
    
    logger.info(f"üîç RAG search completed:")
    logger.info(f"  - Property type: {property_type}")
    logger.info(f"  - Property name: {property_name}")
    logger.info(f"  - Context length: {len(context)} chars")
    logger.info(f"  - Context preview: {context[:200]}...")
    
    # Build the query with context based on property type
    if context:
        # Îß§Î¨º ÌÉÄÏûÖÏóê Îî∞Î•∏ ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
        if property_type == "TYPE_A":
            # ÏÑúÏïàÍ∞úÎ∞ú Î≥¥Ïú† ÏûêÏÇ∞ - ÏßÅÏ†ë ÏÉÅÎã¥ Í∞ÄÎä•
            response_guide = """Ï≤´ Ï§Ñ: [ÏÑúÏïàÍ∞úÎ∞ú Î≥¥Ïú† ÏûêÏÇ∞] {Îß§Î¨ºÎ™Ö}

üìç ÏúÑÏπò: {Ï£ºÏÜå}
üè¢ Í±¥Î¨º: {Ï∏µÏàò, Í∑úÎ™®}
üí∞ Ï°∞Í±¥: {Í∞ÄÍ≤©}
‚ú® ÌäπÏßï: {ÌäπÏßï}

üìû Îß§Îß§ ÏÉÅÎã¥: ÏÑúÏïàÍ∞úÎ∞ú Ïª®ÏÑ§ÌåÖÌåÄ 02-3443-0724"""
        
        else:
            # ÎπÑÏ†úÌú¥ Ï§ëÍ∞úÏÇ¨ Îß§Î¨º - ÏãúÏû• Ï∞∏Í≥† Ï†ïÎ≥¥Î°úÎßå Ï†úÍ≥µ
            response_guide = """Ï≤´ Ï§Ñ: [ÏãúÏû• Ï∞∏Í≥† Ï†ïÎ≥¥] {Íµ¨} {Îèô} ÏùºÎåÄ

‚ö†Ô∏è Ï§ëÏöî: Í±¥Î¨ºÎ™Ö/ÏßÄÎ≤à Ï†àÎåÄ ÎÖ∏Ï∂ú Í∏àÏßÄ! (Î≤ïÏ†Å Ïù¥Ïäà)

Ìï¥Îãπ ÏßÄÏó≠Ïùò ÏãúÏû• Ï†ïÎ≥¥Îßå Ï†úÍ≥µ:

{Îèô}ÏóêÎäî ÏïΩ {ÏµúÏÜå}Ïñµ~{ÏµúÎåÄ}ÏñµÏõêÎåÄ {Ïö©ÎèÑ} Îß§Î¨ºÏù¥ ÏûàÏäµÎãàÎã§.

üí∞ Í∞ÄÍ≤©ÎåÄ: ÏïΩ {ÏµúÏÜå}Ïñµ~{ÏµúÎåÄ}ÏñµÏõêÎåÄ
üè¢ Ïö©ÎèÑ: {Ïö©ÎèÑ1}, {Ïö©ÎèÑ2}
üìê Í∑úÎ™®: {ÏÜåÌòï/Ï§ëÌòï/ÎåÄÌòï}Í∏â

‚ÑπÔ∏è Î≥∏ Ï†ïÎ≥¥Îäî ÏãúÏû• Ï∞∏Í≥†Ïö©Ïù¥Î©∞, Ï†ïÌôïÌïú Îß§Î¨º Ï†ïÎ≥¥Îäî Ï†ÑÎ¨∏Í∞Ä ÏÉÅÎã¥ÏùÑ ÌÜµÌï¥ Î¨∏ÏùòÌï¥Ï£ºÏÑ∏Ïöî.
üìû 02-3443-0724

Ï†àÎåÄ Í∏àÏßÄ:
- Í±¥Î¨ºÎ™Ö Ïñ∏Í∏â (ÏÜåÎã¥ÎπåÎî©, Ìò∏ÏïîÎπåÎî© Îì±)
- ÏßÄÎ≤à Ïñ∏Í∏â (39-7, 111-31 Îì±)
- Íµ¨Ï≤¥Ï†Å Ï£ºÏÜå"""

        query = f"""REXA Î∂ÄÎèôÏÇ∞ Ï†ÑÎ¨∏Í∞Ä. ÏßÄÏó≠ Ï†ïÎ≥¥Îßå Ï†úÍ≥µ.

üö® Í∑úÏπô:
1. Context Ï†ïÎ≥¥Îßå ÏÇ¨Ïö©
2. Í±¥Î¨ºÎ™Ö Ï†àÎåÄ ÎÖ∏Ï∂ú Í∏àÏßÄ! (Î≤ïÏ†Å Ïù¥Ïäà)
3. ÏßÄÎ≤à Ï†àÎåÄ ÎÖ∏Ï∂ú Í∏àÏßÄ!
4. Í∞ÄÍ≤©ÎåÄ Î≤îÏúÑÎ°úÎßå ÌëúÏãú (Ïòà: 130Ïñµ~500Ïñµ)
5. Ïö©ÎèÑÎßå ÌëúÏãú (Í∑ºÎ¶∞ÏÉùÌôúÏãúÏÑ§, ÏóÖÎ¨¥ÏãúÏÑ§ Îì±)
6. Í∞ïÎÇ®Í∂å ÏµúÏÜå 100Ïñµ!
7. "Íº¨ÎßàÎπåÎî©", "ÎπåÎî©" Í∞ôÏùÄ ÏùºÎ∞ò Î™ÖÏÇ¨Îäî Î¨¥ÏãúÌïòÍ≥† ÏßÄÏó≠ Ï†ïÎ≥¥Îßå Ï†úÍ≥µ

Ïòà: "ÎÖºÌòÑÎèô Íº¨ÎßàÎπåÎî©" ‚Üí ÎÖºÌòÑÎèô ÏßÄÏó≠ Ï†ïÎ≥¥ Ï†úÍ≥µ

Type: {property_type} - {property_name}
{response_guide}

Context: {context}

ÏßàÎ¨∏: {prompt}

Í±¥Î¨ºÎ™Ö/ÏßÄÎ≤à Ï†àÎåÄ Í∏àÏßÄ! Í∞ÄÍ≤©ÎåÄÏôÄ Ïö©ÎèÑÎßå!"""
        
        logger.info(f"üîç Using RAG with {len(context)} chars of context")
        logger.info(f"üè∑Ô∏è Property Type: {property_type} ({property_name})")
    
    else:
        query = f"""You are REXA, a chatbot that is a real estate expert with 10 years of experience in taxation (capital gains tax, property holding tax, gift/inheritance tax, acquisition tax), auctions, civil law, and building law. 
Respond politely and with a trustworthy tone, as a professional advisor would.

**ÏùëÎãµ ÌòïÏãù Í∞ÄÏù¥Îìú (Îß§Ïö∞ Ï§ëÏöî):**
- ÏµúÎåÄ 200 ÌÜ†ÌÅ∞ Ïù¥ÎÇ¥Î°ú Í∞ÑÍ≤∞ÌïòÍ≤å ÎãµÎ≥Ä
- ÏûÑÎåÄÏ°∞Í±¥, Í±¥Î¨ºÏ†ïÎ≥¥ Îì± Ï†ïÎ≥¥ÏÑ± ÎÇ¥Ïö©ÏùÄ Î∞òÎìúÏãú ÏöîÏïΩ ÌòïÏãùÏúºÎ°ú Ï†úÍ≥µ
- Î∂àÌïÑÏöîÌïú ÏÑúÏà†Ìòï ÏÑ§Î™ÖÏùÄ ÏµúÏÜåÌôîÌïòÍ≥† ÌïµÏã¨ Ï†ïÎ≥¥Îßå Ï†ÑÎã¨
- Ïà´Ïûê Ï†ïÎ≥¥Îäî Î™ÖÌôïÌïòÍ≥† Í∞ÑÍ≤∞ÌïòÍ≤å ÌëúÏãú


Question: {prompt}

And please respond in Korean following the above format."""
        logger.info("‚ÑπÔ∏è Processing without RAG context")
    
    logger.info(f"ü§ñ Calling Solar API with prompt: {prompt[:50]}...")
    
    try:
        response = client.chat.completions.create(
            model="solar-mini",
            messages=[{"role": "user", "content": query}],
            temperature=0,  # ÏÜçÎèÑ ÏµúÏö∞ÏÑ† (0.1 -> 0)
            max_tokens=400,  # ÏÜçÎèÑ Ìñ•ÏÉÅ (500 -> 400)
            timeout=API_TIMEOUT
        )
        
        answer = response.choices[0].message.content
        logger.info(f"‚úÖ Solar API success - Response length: {len(answer)} chars")
        
        # ==================== ÏùëÎãµ Í≤ÄÏ¶ù (Í∞ÑÏÜåÌôî + ÌïÑÏàò Ï≤¥ÌÅ¨) ====================
        validation_errors = []
        
        # 1. Í∏àÏßÄÎêú ÏßÄÏó≠Î™Ö Í≤ÄÏ¶ù (Îèô Îã®ÏúÑ - FORBIDDEN_LOCATIONSÎäî Ïù¥ÎØ∏ ÎèôÎßå Ìè¨Ìï®)
        forbidden_found = [loc for loc in FORBIDDEN_LOCATIONS if loc in answer]
        if forbidden_found:
            validation_errors.append(f"Í∏àÏßÄ ÏßÄÏó≠: {forbidden_found[0]}")
        
        # 2. Í∏àÏßÄÎêú Í±¥Î¨º Ïö©ÎèÑ Í≤ÄÏ¶ù (Îπ†Î•∏ Ï≤¥ÌÅ¨)
        forbidden_types = [btype for btype in FORBIDDEN_BUILDING_TYPES if btype in answer]
        if forbidden_types:
            validation_errors.append(f"Í∏àÏßÄ Ïö©ÎèÑ: {forbidden_types[0]}")
        
        # 3. Îπ†Î•∏ Í∞ÄÍ≤©ÎåÄ Ï≤¥ÌÅ¨ (Ï†ïÍ∑úÏãù ÏóÜÏù¥ Í∞ÑÎã®ÌïòÍ≤å)
        if "Ï≤≠Îã¥Îèô" in answer or "ÎÖºÌòÑÎèô" in answer or "ÎåÄÏπòÎèô" in answer:
            # Í∞ïÎÇ® 3Íµ¨Îäî ÏµúÏÜå 100Ïñµ Ïù¥ÏÉÅ
            if "20Ïñµ" in answer or "30Ïñµ" in answer or "50Ïñµ" in answer or "80Ïñµ" in answer:
                validation_errors.append(f"Í∞ÄÍ≤© Ïò§Î•ò: Í∞ïÎÇ®Í∂åÏùÄ ÏµúÏÜå 100Ïñµ Ïù¥ÏÉÅ")
        
        # 4. ÏßàÎ¨∏-ÏùëÎãµ ÏßÄÏó≠ Î∂àÏùºÏπò Í∞ÑÎã® Ï≤¥ÌÅ¨ (ÏÜçÎèÑ Ï§ëÏöî)
        if "ÎÖºÌòÑÎèô" in prompt and "Ï≤≠Îã¥Îèô" in answer and "ÎÖºÌòÑÎèô" not in answer:
            validation_errors.append(f"ÏßÄÏó≠ Î∂àÏùºÏπò: ÎÖºÌòÑÎèô ÏßàÎ¨∏ ‚Üí Ï≤≠Îã¥Îèô ÏùëÎãµ")
        elif "Ï≤≠Îã¥Îèô" in prompt and "ÎÖºÌòÑÎèô" in answer and "Ï≤≠Îã¥Îèô" not in answer:
            validation_errors.append(f"ÏßÄÏó≠ Î∂àÏùºÏπò: Ï≤≠Îã¥Îèô ÏßàÎ¨∏ ‚Üí ÎÖºÌòÑÎèô ÏùëÎãµ")
        elif "Ïã†ÎÇ¥Îèô" in prompt and ("Ï≤≠Îã¥Îèô" in answer or "ÎÖºÌòÑÎèô" in answer):
            validation_errors.append(f"ÏßÄÏó≠ Î∂àÏùºÏπò: Ïã†ÎÇ¥Îèô ÏßàÎ¨∏ ‚Üí Í∞ïÎÇ® ÏùëÎãµ")
        
        # Í≤ÄÏ¶ù Ïã§Ìå® Ïãú ÏóêÎü¨ ÏùëÎãµ
        if validation_errors:
            logger.error(f"üö® VALIDATION FAILED: {validation_errors[0]}")
            
            return {
                "version": "2.0",
                "template": {
                    "outputs": [{
                        "simpleText": {
                            "text": f"""Ï£ÑÏÜ°Ìï©ÎãàÎã§. Ìï¥Îãπ Ï†ïÎ≥¥Îäî ÌòÑÏû¨ Î≥¥Ïú†ÌïòÍ≥† ÏûàÏßÄ ÏïäÏäµÎãàÎã§.

Ï†ÑÏ≤¥ Îß§Î¨º Î™©Î°ù: "Îß§Î¨º Ï∂îÏ≤ú" ÎòêÎäî "Î¶¨Ïä§Ìä∏" ÏöîÏ≤≠

üìû ÏÉÅÎã¥: ÏÑúÏïàÍ∞úÎ∞ú Ïª®ÏÑ§ÌåÖÌåÄ 02-3443-0724"""
                        }
                    }]
                }
            }
        
        logger.info(f"‚úÖ Validation passed")
        logger.info(f"üì§ Sending response: {answer[:100]}...")
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": answer
                        }
                    }
                ]
            }
        }
        
    except APITimeoutError as e:
        logger.error(f"‚è∞ API Timeout after {API_TIMEOUT}s: {e}")
        raise
    except OpenAIError as e:
        logger.error(f"‚ùå OpenAI API Error: {e}")
        raise
    except Exception as e:
        logger.error(f"‚ùå Unexpected error: {type(e).__name__}: {e}")
        raise

# ================================================================================
# API Endpoints
# ================================================================================

@app.get("/")
def read_root():
    return {"Hello": "REXA - Real Estate Expert Assistant (Solar + RAG)"}

@app.post("/generate")
async def generate_text(request: RequestBody):
    """REXA Î∂ÄÎèôÏÇ∞ Ï†ÑÎ¨∏ Ï±óÎ¥á with RAG - /generate ÏóîÎìúÌè¨Ïù∏Ìä∏"""
    request_id = str(uuid.uuid4())
    
    logger.info("="*50)
    logger.info(f"üì® New request received at /generate: {request_id[:8]}")
    logger.info(f"üìã Full request body: {request.model_dump()}")
    
    try:
        # 3Ï¥à ÌÉÄÏûÑÏïÑÏõÉÏúºÎ°ú Îπ†Î•∏ ÏùëÎãµ ÏãúÎèÑ
        result = await process_solar_rag_request(request.model_dump())
        logger.info(f"‚úÖ Request {request_id[:8]} completed successfully")
        return result
        
    except APITimeoutError as e:
        logger.warning(f"‚è∞ Timeout (3s) - enqueueing request {request_id}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "ÎãµÎ≥Ä ÏÉùÏÑ±Ïóê ÏãúÍ∞ÑÏù¥ Í±∏Î¶¨Í≥† ÏûàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏßàÎ¨∏Ìï¥Ï£ºÏÑ∏Ïöî."
                        }
                    }
                ]
            }
        }
        
    except OpenAIError as e:
        logger.error(f"‚ùå API Error: {e}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "ÏùºÏãúÏ†ÅÏù∏ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî."
                        }
                    }
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error: {type(e).__name__}: {e}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "Ï£ÑÏÜ°Ìï©ÎãàÎã§. Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Îã§Ïãú ÌïúÎ≤à ÏßàÎ¨∏Ìï¥Ï£ºÏãúÍ≤†Ïñ¥Ïöî?"
                        }
                    }
                ]
            }
        }

@app.post("/custom")
async def generate_custom(request: RequestBody):
    """REXA Î∂ÄÎèôÏÇ∞ Ï†ÑÎ¨∏ Ï±óÎ¥á with RAG - Ïπ¥Ïπ¥Ïò§ÌÜ° 5Ï¥à Ï†úÌïú ÎåÄÏùë"""
    request_id = str(uuid.uuid4())
    
    logger.info("="*50)
    logger.info(f"üì® New RAG request received: {request_id[:8]}")
    logger.info(f"üìã Full request body: {request.model_dump()}")
    
    try:
        # 3Ï¥à ÌÉÄÏûÑÏïÑÏõÉÏúºÎ°ú Îπ†Î•∏ ÏùëÎãµ ÏãúÎèÑ
        result = await process_solar_rag_request(request.model_dump())
        logger.info(f"‚úÖ Request {request_id[:8]} completed successfully")
        return result
        
    except APITimeoutError as e:
        logger.warning(f"‚è∞ Timeout (3s) - enqueueing request {request_id}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "ÎãµÎ≥Ä ÏÉùÏÑ±Ïóê ÏãúÍ∞ÑÏù¥ Í±∏Î¶¨Í≥† ÏûàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏßàÎ¨∏Ìï¥Ï£ºÏÑ∏Ïöî."
                        }
                    }
                ]
            }
        }
        
    except OpenAIError as e:
        logger.error(f"‚ùå API Error: {e}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "ÏùºÏãúÏ†ÅÏù∏ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Ïû†Ïãú ÌõÑ Îã§Ïãú ÏãúÎèÑÌï¥Ï£ºÏÑ∏Ïöî."
                        }
                    }
                ]
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error: {type(e).__name__}: {e}")
        await enqueue_webhook_request(request_id, request.model_dump())
        
        return {
            "version": "2.0",
            "template": {
                "outputs": [
                    {
                        "simpleText": {
                            "text": "Ï£ÑÏÜ°Ìï©ÎãàÎã§. Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. Îã§Ïãú ÌïúÎ≤à ÏßàÎ¨∏Ìï¥Ï£ºÏãúÍ≤†Ïñ¥Ïöî?"
                        }
                    }
                ]
            }
        }

@app.get("/health")
async def health_check() -> HealthStatus:
    """Enhanced health check endpoint"""
    queue_size, processing_size, failed_size = await get_queue_sizes()
    
    return HealthStatus(
        status="healthy" if server_healthy else "unhealthy",
        model="solar-mini",
        mode="rexa_chatbot_rag",
        server_healthy=server_healthy,
        last_check=last_health_check.isoformat(),
        redis_connected=(redis_client is not None and not use_in_memory_queue),
        queue_size=queue_size,
        processing_queue_size=processing_size,
        failed_queue_size=failed_size
    )

@app.get("/health/ping")
async def health_ping():
    """Simple ping endpoint for client health checks"""
    return {
        "alive": True,
        "healthy": server_healthy,
        "timestamp": datetime.now().isoformat(),
        "rag_enabled": len(chunk_embeddings) > 0
    }

@app.get("/queue/status")
async def queue_status():
    """Get detailed queue status"""
    queue_size, processing_size, failed_size = await get_queue_sizes()
    
    return {
        "queue_type": "in-memory" if use_in_memory_queue else "redis",
        "webhook_queue": queue_size,
        "processing_queue": processing_size,
        "failed_queue": failed_size,
        "total": queue_size + processing_size + failed_size,
        "rag_chunks_loaded": len(article_chunks)
    }

@app.post("/queue/retry-failed")
async def retry_failed_requests():
    """Manually retry all failed requests"""
    global in_memory_failed_queue, in_memory_webhook_queue  # Ï†ÑÏó≠ Î≥ÄÏàò ÏÑ†Ïñ∏
    try:
        if use_in_memory_queue:
            retry_count = len(in_memory_failed_queue)
            while len(in_memory_failed_queue) > 0:
                req = in_memory_failed_queue.pop()
                req.retry_count = 0
                in_memory_webhook_queue.appendleft(req)
            
            logger.info(f"‚úÖ Retrying {retry_count} failed requests (in-memory)")
            return {"retried": retry_count, "queue_type": "in-memory"}
        
        if not redis_client:
            return {"error": "Queue not available"}
        
        failed_items = await redis_client.lrange(WEBHOOK_FAILED_QUEUE, 0, -1)
        retry_count = 0
        
        for item in failed_items:
            req = QueuedRequest.model_validate_json(item)
            req.retry_count = 0
            await redis_client.lpush(WEBHOOK_QUEUE_NAME, req.model_dump_json())
            retry_count += 1
        
        await redis_client.delete(WEBHOOK_FAILED_QUEUE)
        
        logger.info(f"‚úÖ Retrying {retry_count} failed requests (Redis)")
        return {"retried": retry_count, "queue_type": "redis"}
        
    except Exception as e:
        logger.error(f"‚ùå Failed to retry requests: {e}")
        return {"error": str(e)}

# ================================================================================
# Startup & Shutdown Events
# ================================================================================

def preprocess_query(query: str) -> str:
    """
    ÏßàÎ¨∏ Ï†ÑÏ≤òÎ¶¨: Ïó≠Î™ÖÏùÑ ÏßÄÏó≠Î™ÖÏúºÎ°ú Î≥ÄÌôò
    
    Ïòà:
    - "Í∞ïÎÇ®Íµ¨Ï≤≠Ïó≠ Í∑ºÏ≤ò" ‚Üí "Ï≤≠Îã¥Îèô ÎÖºÌòÑÎèô Í∑ºÏ≤ò"
    - "ÏÑ†Î¶âÏó≠ Ìà¨Ïûê" ‚Üí "Ïó≠ÏÇºÎèô ÎåÄÏπòÎèô Ìà¨Ïûê"
    """
    processed = query
    
    # Ïó≠Î™Ö ‚Üí ÏßÄÏó≠Î™Ö Î≥ÄÌôò
    for station, locations in STATION_TO_LOCATION.items():
        if station in query:
            # "Í∞ïÎÇ®Íµ¨Ï≤≠Ïó≠" ‚Üí "Ï≤≠Îã¥Îèô ÎÖºÌòÑÎèô"
            location_str = " ".join(locations)
            processed = processed.replace(station, location_str)
            logger.info(f"üîÑ Station mapping: {station} ‚Üí {location_str}")
    
    return processed

@app.on_event("startup")
async def startup_event():
    """Initialize resources on startup"""
    logger.info("="*70)
    logger.info("üöÄ Starting REXA server (Solar + RAG + Property Type Detection)...")
    logger.info("="*70)
    
    # RAG ÏÉÅÌÉú ÌôïÏù∏
    if len(chunk_embeddings) > 0:
        logger.info(f"‚úÖ RAG ENABLED: {len(chunk_embeddings)} chunks loaded")
        logger.info(f"‚úÖ Metadata loaded: {len(chunk_metadata)} entries")
    else:
        logger.warning("‚ö†Ô∏è RAG DISABLED: No embeddings loaded")
        logger.warning("‚ö†Ô∏è Server will work but without company-specific knowledge")
    
    # Redis Ï¥àÍ∏∞Ìôî
    await init_redis()
    
    # Background tasks
    asyncio.create_task(health_check_monitor())
    asyncio.create_task(queue_processor())
    
    logger.info("="*70)
    logger.info("‚úÖ REXA server startup complete!")
    logger.info(f"   - Model: solar-mini")
    logger.info(f"   - RAG chunks: {len(chunk_embeddings)}")
    logger.info(f"   - Metadata entries: {len(chunk_metadata)}")
    logger.info(f"   - Redis: {'connected' if redis_client else 'in-memory queue'}")
    logger.info("="*70)

@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup resources on shutdown"""
    logger.info("üëã Shutting down REXA server (Solar + RAG)...")
    await close_redis()
    logger.info("‚úÖ REXA server shut down successfully")
